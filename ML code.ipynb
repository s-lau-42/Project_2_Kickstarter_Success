{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the confusion matrix for each classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for i,model in enumerate([clf_A,clf_B,clf_C]):\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the data\n",
    "\n",
    "    # view with a heatmap\n",
    "    plt.figure(i)\n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"size\":30}, \n",
    "            cmap='Blues', square=True, fmt='.3f')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion matrix for:\\n{}'.format(model.__class__.__name__));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomized search for model parameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random = RandomizedSearchCV(estimator=lr, param_distributions=param_grid, cv = 3, n_jobs=-1)\n",
    "\n",
    "random_result = random.fit(X, y)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the supervised model on the training set  \n",
    "model = AdaBoostClassifier().fit(X_train,y_train)\n",
    "\n",
    "# Extract the feature importances\n",
    "importances = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1294dd549e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstd_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#std_scale = preprocessing.StandardScaler().fit(df_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_train_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "#std scaler\n",
    "std_scale = preprocessing.StandardScaler().fit(df_test)\n",
    "df_train_test = std_scale.transform(df_test)\n",
    "df_scale = pd.DataFrame(df_train_test)\n",
    "df_scale.columns = df_test.columns\n",
    "df_scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca\n",
    "pca = PCA()\n",
    "pca.fit_transform(X)\n",
    "explained_var = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(explained_var > 0.8)[0][0])\n",
    "np.where(explained_var > 0.9)[0][0])\n",
    "np.where(explained_var > 0.99)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log with pca values\n",
    "logreg_start = time.time()\n",
    "\n",
    "pipe_logreg = Pipeline([('pca', PCA(n_components=90)),\n",
    "                    ('clf', LogisticRegression())])\n",
    "\n",
    "params_logreg = [\n",
    "    {'clf__penalty': ['l1', 'l2'],\n",
    "     'clf__fit_intercept': [True, False],\n",
    "        'clf__C': [0.001, 0.01, 1, 10]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_logreg = GridSearchCV(estimator=pipe_logreg,\n",
    "                  param_grid=params_logreg,\n",
    "                  cv=5)\n",
    "\n",
    "grid_logreg.fit(X_train, y_train)\n",
    "\n",
    "logreg_end = time.time()\n",
    "\n",
    "logreg_best_score = grid_logreg.best_score_\n",
    "logreg_best_params = grid_logreg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnd forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               random_state=RSEED, \n",
    "                               max_features = 'sqrt',\n",
    "                               n_jobs=-1, verbose = 1)\n",
    "\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "n_nodes = []\n",
    "max_depths = []\n",
    "\n",
    "# Stats about the trees in random forest\n",
    "for ind_tree in model.estimators_:\n",
    "    n_nodes.append(ind_tree.tree_.node_count)\n",
    "    max_depths.append(ind_tree.tree_.max_depth)\n",
    "    \n",
    "print(f'Average number of nodes {int(np.mean(n_nodes))}')\n",
    "print(f'Average maximum depth {int(np.mean(max_depths))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
